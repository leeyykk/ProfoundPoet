{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2008,"status":"ok","timestamp":1685493286878,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"yS-1lSTa7rEW","outputId":"55c700ba-38c4-444c-e33a-26dc22deabdd"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package cmudict to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/cmudict.zip.\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('cmudict')\n","nltk.download('brown')\n","nltk.download('stopwords')\n","\n","#need tagger to extract data of no themes\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244575,"status":"ok","timestamp":1685493531451,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"f7pdmzIu7vQO","outputId":"f8f928d0-cb39-4b97-a48b-b4c0bac83ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import csv\n","import re\n","from nltk import word_tokenize\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import cmudict\n","from nltk.corpus import brown\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","from collections import Counter\n","\n","from google.colab import drive \n","drive.mount('/content/gdrive')\n","\n","data = pd.read_csv('gdrive/My Drive/ProfoundPoet/PoetryFoundationData.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685493531451,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"VrlkSzyL-9ty"},"outputs":[],"source":["poems = data['Poem']\n","themes = data['Tags']\n","titles = data['Title']"]},{"cell_type":"markdown","metadata":{"id":"Irwba5Dl_AXy"},"source":["## 1. We  will need trigrams (sequential data) of all poems and brown corpus words in order to generate our poems"]},{"cell_type":"markdown","metadata":{"id":"00Y49aKd_Ti_"},"source":["### Brown Trigrams:\n","We need to extract trigrams with the sentence boundaries"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5740,"status":"ok","timestamp":1685493537188,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"aI2LerGr-_ki"},"outputs":[],"source":["brown_trigrams = [tri for sent in brown.sents() for tri in nltk.ngrams(sent, 3)]\n","brown_cfd = nltk.ConditionalFreqDist(((tri[0].lower(),tri[1].lower()), tri[-1].lower()) for tri in brown_trigrams)"]},{"cell_type":"markdown","metadata":{"id":"UZbI-4iK_WM2"},"source":["###Poem Trigrams:\n","\n","This will take us some time, as we want to extract the trigrams in units of 'line' but our poems need to be tokenized into lines and tokens"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1685493537188,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"x7IXbatJ_7mc"},"outputs":[],"source":["# From the test case above, created a function to tokenize all poems in the dataset\n","def tokenize_poem_by_line(poem):\n","  # want to maintain the line splitting/division within poems\n","  poem_split = re.split(r'[\\n\\r]+| + ', poem)\n","  # want to only work with the words in the poem\n","  return [re.findall(r'[a-zA-Z0-9\\']+', line) for line in poem_split if line!='' and line!=' ']"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2603,"status":"ok","timestamp":1685493539779,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"FZ2tnISeAIq2"},"outputs":[],"source":["#tokenizing all poems\n","tokenized_poems = [tokenize_poem_by_line(poem) for poem in poems]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9254,"status":"ok","timestamp":1685493549030,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"wap854EiAa1m"},"outputs":[],"source":["poem_trigrams = [tri for poem in tokenized_poems for line in poem for tri in nltk.ngrams(line, 3)]\n","poem_cfd = nltk.ConditionalFreqDist(((tri[0].lower(),tri[1].lower()), tri[-1].lower()) for tri in poem_trigrams)"]},{"cell_type":"markdown","metadata":{"id":"x8AEX_sdNFS4"},"source":["###Check formation"]},{"cell_type":"markdown","metadata":{"id":"E5HatWiDNJvw"},"source":["Let's check if your trigrams have been created properly"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1685493551970,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"ITJj6n6hNJNY","outputId":"f0e1a398-630c-4835-8b46-0ba75f840fe5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example entry in brown: (('the', 'fulton'), FreqDist({'county': 3, \"ordinary's\": 1, 'tax': 1, 'health': 1})) , Length of brown_cfd: 414945\n","Example entry in poem: (('dog', 'bone'), FreqDist({'stapler': 1})) , Length of poem_cfd: 993376\n"]}],"source":["print(\"Example entry in brown:\", list(brown_cfd.items())[0], \", Length of brown_cfd:\", len(brown_cfd.conditions()))\n","print(\"Example entry in poem:\",list(poem_cfd.items())[0], \", Length of poem_cfd:\", len(poem_cfd.conditions()))"]},{"cell_type":"markdown","metadata":{"id":"ZbOSdxI-Ofc1"},"source":["##Getting pair of tokens that come at the end of a poem's line (reference for end-point)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685493551971,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"QjEFZMEvOn0R"},"outputs":[],"source":["def get_end_tokens():\n","  return nltk.FreqDist((line[-2],line[-1]) for poem in tokenized_poems for line in poem if len(line)>2)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":514,"status":"ok","timestamp":1685493552482,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"Qw-sBX3cOxUf"},"outputs":[],"source":["line_endings = get_end_tokens()\n","\n","common_line_endings = [first for first, second in line_endings.most_common(3000)]"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685493552482,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"PEs1F86aO24I","outputId":"eb22d7ff-5a43-45c6-b24d-dc223e6ada49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 'world'),\n"," ('to', 'me'),\n"," ('the', 'sun'),\n"," ('the', 'sky'),\n"," ('the', 'sea')]"]},"metadata":{},"execution_count":11}],"source":["common_line_endings[:5]"]},{"cell_type":"markdown","metadata":{"id":"aikJmbrTAseb"},"source":["##Grouping / Identifying frequently appearing words by theme"]},{"cell_type":"markdown","metadata":{"id":"WqWiUz0dBLVU"},"source":["###Assigning poems with no tag with tags inferred from their titles"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7635,"status":"ok","timestamp":1685493560115,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"72i-PgAPBSY5"},"outputs":[],"source":["stopwords = nltk.corpus.stopwords.words()\n","\n","unique_themes = set(taglist for taglist in themes if isinstance(taglist,str))\n","unique_themes = set(theme for taglist in unique_themes for theme in re.split(r',\\s*|\\s*&\\s*|\\s+',taglist) if theme and theme.lower() not in stopwords)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1685493560116,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"cI-Vo9xgnhVs","outputId":"dd7c2f64-214a-4677-87f6-87fb7513b605"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["180"]},"metadata":{},"execution_count":13}],"source":["len(unique_themes)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1685493560116,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"YIeSKtTXBYRB"},"outputs":[],"source":["lemmatizer = nltk.WordNetLemmatizer()\n","\n","# Look for synset names, which are a generalization or representation of words, in the hypernym path to see if there are any overlaps with the unique themes\n","\n","#Function that extracts the synset names of the hypernyms\n","def hypernym_names(synsets):\n","  return [word for synset in synsets for syn in synset.hypernym_paths()[0] for word in re.split(r'_', syn.name()[:-5])]\n","\n","def no_tag(title):\n","  #Extract the words from the title\n","  keywords = re.findall(r'[a-zA-Z]+',title)\n","  #There are many synsets given one word, so to extract relevant synsets (as much as possible) we only gather the synsets of the same pos type\n","  keyword_pos_tags = nltk.pos_tag(keywords)\n","\n","  #we collect keywords as they are if they are in the form of a noun, verb, or adjective in case there are no overlaps with the unique themes\n","  keywords = set()\n","  synsets = []\n","  for keyword, pos in keyword_pos_tags:\n","    if pos[0] == 'N':\n","      keywords.add(keyword)\n","      synsets += wn.synsets(keyword, pos = wn.NOUN) \n","    elif pos[0] == 'V':\n","      keywords.add(keyword)\n","      synsets += wn.synsets(keyword, pos = wn.VERB)\n","    elif pos[0] == 'J':\n","      keywords.add(keyword)\n","      synsets += wn.synsets(keyword, pos = wn.ADJ)\n","    else:\n","      continue\n","  \n","  names = hypernym_names(synsets)\n","  manual = set()\n","\n","  for name in names:\n","    for theme in unique_themes:\n","      lemma = lemmatizer.lemmatize(theme.lower())\n","      if name.lower() == lemma: manual.add(theme.lower())\n","  \n","  if not manual:\n","    return list(keywords)\n","\n","  return list(manual)"]},{"cell_type":"markdown","metadata":{"id":"Yjzu5GG3A8vT"},"source":["### Tokenizing themes in the poem"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":35964,"status":"ok","timestamp":1685493596069,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"Zf3wkBJVA6ng"},"outputs":[],"source":["#Here, I extracted the tags and tokenized them and added the list of tags per poem to the list 'themes'\n","poem_themes = []\n","for i, tag_list in enumerate(themes):\n","  if isinstance(tag_list, str):\n","    poem_themes.append(re.split(r',\\s*|\\s*&\\s*',tag_list))\n","  else:\n","    poem_themes.append(no_tag(titles[i]))"]},{"cell_type":"markdown","metadata":{"id":"9nYLpnm6Bpuk"},"source":["###Finding the frequently appearing words"]},{"cell_type":"markdown","metadata":{"id":"9ObWDyZHCGwX"},"source":["We only want to inspect words that are nouns, verbs, or adjectives so we use the nltk pos_tagger."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685493596069,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"mfcDLdvt7wUe"},"outputs":[],"source":["def unique_words_in_poem(poem):\n","  words = [word.lower() for word in re.findall(r'[a-zA-Z\\']+', poem) if word not in stopwords]\n","  pos_words =  nltk.pos_tag(words)\n","  return nltk.FreqDist(word.lower() for word,tag in pos_words if tag[0] in [\"N\",\"V\",\"J\"])"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":300070,"status":"ok","timestamp":1685493896130,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"Tr6VM5Iq8IlL"},"outputs":[],"source":["unique_words_poem = [unique_words_in_poem(poem) for poem in poems]"]},{"cell_type":"markdown","metadata":{"id":"qM9OlAMcCZ7I"},"source":["###Associating the Frequency Distributions of each poem with their theme tags"]},{"cell_type":"markdown","metadata":{"id":"yGdu5BspCm-D"},"source":["First, associating the unique words with all themes the poem is associated with"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1685493896132,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"a8JzuqQJ8T66"},"outputs":[],"source":["unique_words_dataset = list(zip(unique_words_poem, poem_themes))"]},{"cell_type":"markdown","metadata":{"id":"iLlisTOrCvKe"},"source":["Next, creating a dictionary that has key of an individual theme, and value of a frequency distribution of unique words.\n","We will use a lemmatizer for the keys for consistency"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":8843,"status":"ok","timestamp":1685493904956,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"b3pOXfck8WZ_"},"outputs":[],"source":["group_words_by_theme = defaultdict(nltk.FreqDist)\n","\n","for (fdist, themes) in unique_words_dataset:\n","  for theme in themes:\n","    if theme:\n","      key = lemmatizer.lemmatize(theme.lower())\n","      group_words_by_theme[key].update(fdist)"]},{"cell_type":"markdown","metadata":{"id":"RW_f4rrnDDM7"},"source":["Let's check if the dictionary was well-created"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685493904957,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"3MV4PvBp-E9v","outputId":"0c1334ef-0f1c-4caa-c137-81701b45f334"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'i': 1857, 'day': 177, 'white': 166, 'time': 147, 'black': 135, 'world': 134, 'night': 129, \"an'\": 126, 'light': 120, 'eyes': 118, ...})"]},"metadata":{},"execution_count":20}],"source":["group_words_by_theme['class']"]},{"cell_type":"code","source":["theme_list = [key for key in group_words_by_theme.keys() if group_words_by_theme[key].N() > 180000]\n","theme_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdmjSgOqqTBn","executionInfo":{"status":"ok","timestamp":1685500708711,"user_tz":-540,"elapsed":4,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"}},"outputId":"5b9cb671-71d0-404c-fcd6-f4529da87b4b"},"execution_count":233,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['science',\n"," 'time',\n"," 'religion',\n"," 'activity',\n"," 'history',\n"," 'living',\n"," 'relationship',\n"," 'nature',\n"," 'social commentaries',\n"," 'politics',\n"," 'death',\n"," 'love',\n"," 'art']"]},"metadata":{},"execution_count":233}]},{"cell_type":"markdown","metadata":{"id":"KcTarDbpDJkk"},"source":["Let's extract the frequent words that occur more than COMMON_COUNT times in the theme. The reason why we set a threshold is because we want to remove words that only appear in certain poems (like proper nouns)\n","\n","eg) Theme : Love\n","\n","I rode a ferrari to get to her -> 'horse' is not relevant to the theme 'love'"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685493904957,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"EpYpW2gD-cAF"},"outputs":[],"source":["COMMON_COUNT = 20\n","\n","def get_frequent_words(theme, min_count = COMMON_COUNT):\n","  return [word for word,count in group_words_by_theme[theme].most_common() if count > min_count]"]},{"cell_type":"markdown","metadata":{"id":"1LUbQRVSDlTm"},"source":["Let's check if the common_words are well selected and well reflect the theme"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685493904957,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"-YBjfD45-rNq","outputId":"be384d94-8d77-4e43-a468-16dc0de9ae30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words that occure more than 20 times: 1805\n","Top 20 most occuring words: ['i', 'love', 'heart', 'night', 'eyes', 'time', 'light', 'thy', 'day', 'life', 'thou', 'hand', 'make', 'body', 'world', 'sweet', 'made', 'white', 'thee', 'things']\n"]}],"source":["common = get_frequent_words(\"love\")\n","\n","print(\"Number of words that occure more than\", COMMON_COUNT, \"times:\", len(common))\n","print(\"Top 20 most occuring words:\", common[:20])"]},{"cell_type":"markdown","metadata":{"id":"YTw0WcIVFbJQ"},"source":["##Looking for words in freq_words that start with the alliteration letter"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685493904958,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"9ad5DXms-s8-"},"outputs":[],"source":["def alliteration_freq_words(letter, frequent_words):\n","  return [word for word in frequent_words if word[0].startswith(letter)]"]},{"cell_type":"markdown","metadata":{"id":"wnJLoTV6GE-N"},"source":["Let's check if the alliteration are well selected"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685493904958,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"VXIVDFCwGCOs","outputId":"0246f7ca-c97d-485c-9be4-aa331d6d1569"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of frequent words that start with the letter, 'a':  51\n","Top 20 most occuring words: ['i', 'love', 'heart', 'night', 'eyes', 'time', 'light', 'thy', 'day', 'life', 'thou', 'hand', 'make', 'body', 'world', 'sweet', 'made', 'white', 'thee', 'things']\n"]}],"source":["alliteration_words = alliteration_freq_words(\"a\", common)\n","\n","print(\"Number of frequent words that start with the letter, 'a': \", len(alliteration_words))\n","print(\"Top 20 most occuring words:\", common[:20])"]},{"cell_type":"markdown","metadata":{"id":"ipHbtA1MFxp1"},"source":["##Looking for words with a certain rhythm"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":1552,"status":"ok","timestamp":1685493906499,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"aYePtcBrGoET"},"outputs":[],"source":["cmu = cmudict.dict()\n","pron_dict = dict((key, ''.join(value)) for (key,values) in cmu.items() for value in values)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685493906500,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"O0S4VpqkFwzT"},"outputs":[],"source":["def iambic_stress_match(pron):\n","  if [char for char in pron if char.isdigit()] == ['0', '1']: \n","    return True\n","  return False\n","\n","def trochaic_stress_match(pron):\n","  if [char for char in pron if char.isdigit()] == ['1', '0']: \n","    return True\n","  return False\n","\n","def dactylic_stress_match(pron):\n","  if [char for char in pron if char.isdigit()] == ['1', '0', '0']: \n","    return True\n","  return False\n","  \n","def anapestic_stress_match(pron):\n","  if [char for char in pron if char.isdigit()] == ['0', '0', '1']: \n","    return True\n","  return False"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685493906500,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"SzV-zoWnGo0t"},"outputs":[],"source":["def rhythm_word_candidates(func, common):\n","  collection = []\n","  for word in cmu.keys():\n","    if func(pron_dict[word]) and word in common:\n","      collection.append(word)\n","  return collection"]},{"cell_type":"markdown","metadata":{"id":"3uJTc6PRHBra"},"source":["Let's check if the alliteration are well selected"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1685493906875,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"kcuhAu_KGw5o","outputId":"0ad28a99-93fa-471e-ef84-af1aa0632541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of frequent words with a dactylic sound 36\n","Top 20 most occuring words: ['animal', 'animals', 'beautiful', 'carrying', 'century', 'company', 'curious', 'delicate', 'difficult', 'ecstasy', 'gathering', 'glorious', 'happiness', 'images', 'infinite', 'innocence', 'liberty', 'loneliness', 'memories', 'memory']\n"]}],"source":["rhythm_words = rhythm_word_candidates(dactylic_stress_match, common)\n","\n","print(\"Number of frequent words with a dactylic sound\", len(rhythm_words))\n","print(\"Top 20 most occuring words:\", rhythm_words[:20])"]},{"cell_type":"markdown","metadata":{"id":"cjVjePp4HX4r"},"source":["##Synonyms of the theme using Wordnet lexicon"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"Ou5-ySs6HOw-"},"outputs":[],"source":["def get_theme_synonyms(theme):\n","  synsets = wn.synsets(theme)\n","  synonyms = set(re.sub(r'_', ' ', lemma) for syn in synsets for lemma in syn.lemma_names())\n","  return synonyms"]},{"cell_type":"markdown","metadata":{"id":"Y478VFLAHvpA"},"source":["Let's check if the synonyms are well selected"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"c3oHC57KHuEw","outputId":"c18f35c3-9709-4e91-e24e-769ce523e558"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of theme synonyms 36\n","Display some theme synonyms: ['have sex', 'honey', 'make love', 'beloved', 'hump', 'jazz', 'sexual love', 'dear', 'do it', 'enjoy', 'erotic love', 'have it away', 'fuck', 'love life', 'bed', 'making love', 'lovemaking', 'sleep together', 'love', 'have intercourse']\n"]}],"source":["theme_synonyms = get_theme_synonyms('love')\n","\n","print(\"Number of theme synonyms\", len(theme_synonyms))\n","print(\"Display some theme synonyms:\", list(theme_synonyms)[:20])"]},{"cell_type":"markdown","metadata":{"id":"AijLMjbzJPH_"},"source":["##Check if the group of key words identified has any overlaps"]},{"cell_type":"markdown","metadata":{"id":"-j9kQwqgJTMi"},"source":["1. alliteration & rhythm"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"c-I3yyH3H93A","outputId":"1252e362-cbaa-4afb-f20a-feb8d5ed2403"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'animal', 'animals'}"]},"metadata":{},"execution_count":31}],"source":["aliteration_rhythm_intersect = set(alliteration_words).intersection(set(rhythm_words))\n","aliteration_rhythm_intersect"]},{"cell_type":"markdown","metadata":{"id":"RBS5DQ9xJ-b8"},"source":["2. alliteration & synonyms"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"7uRtkF49JiVU","outputId":"bd44c76b-2195-4387-c278-d71d21411186"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["set()"]},"metadata":{},"execution_count":32}],"source":["aliteration_synonyms_intersect = set(alliteration_words).intersection(set(theme_synonyms))\n","aliteration_synonyms_intersect"]},{"cell_type":"markdown","metadata":{"id":"uOUhuRqGKLL9"},"source":["3. rhythm & synonyms"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"beykp85gKHRN","outputId":"7834e5d0-32e6-486f-f8ff-e1b3f396907a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["set()"]},"metadata":{},"execution_count":33}],"source":["rhythm_synonyms_intersect = set(rhythm_words).intersection(set(theme_synonyms))\n","rhythm_synonyms_intersect.union(set(word for word in rhythm_words for synonym in theme_synonyms if word in synonym))\n","rhythm_synonyms_intersect"]},{"cell_type":"markdown","metadata":{"id":"7Iqt_4LPLZxu"},"source":["4. alliteration & rhythm & synonyms"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685493906876,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"TjhfMqGfLHVS","outputId":"674dde71-e279-4920-c42b-b3c0cb150a81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["set()"]},"metadata":{},"execution_count":34}],"source":["alliteration_rhythm_synonyms_intersect = (set(rhythm_words).intersection(set(theme_synonyms))).intersection(set(alliteration_words))\n","alliteration_rhythm_synonyms_intersect"]},{"cell_type":"markdown","metadata":{"id":"rziqm2RNLrx6"},"source":["###Our hierarchy of search-order is 4>(1,2,3)"]},{"cell_type":"markdown","metadata":{"id":"x8idCh5bPA9B"},"source":["##Generation"]},{"cell_type":"code","execution_count":285,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7363,"status":"ok","timestamp":1685501948468,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"WWCWXewIcxEv","outputId":"0c8ca06b-25ce-42fb-af23-85deb9a3f076"},"outputs":[{"output_type":"stream","name":"stdout","text":["These are the themes ['science', 'time', 'religion', 'activity', 'history', 'living', 'relationship', 'nature', 'social commentaries', 'politics', 'death', 'love', 'art']\n","Theme of poem:politics\n","You have selected theme: politics\n","These are the possible topics you can start your poem with:\n","\n","political sympathies\n","politics            political relation  political science   \n","government          "]}],"source":["print(\"These are the themes\", theme_list)\n","theme = input(\"Theme of poem:\")\n","print(\"You have selected theme:\", theme)\n","\n","theme_synonyms = get_theme_synonyms(theme)\n","\n","print(\"These are the possible topics you can start your poem with:\")\n","print()\n","\n","for (i, word) in enumerate(theme_synonyms):\n","  print(\"{:<20}\".format(word), end= \"\")\n","  if i%3 == 0 and i!=1 : print()"]},{"cell_type":"code","execution_count":286,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3776,"status":"ok","timestamp":1685501953841,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"dAk1jhfMhbeV","outputId":"6f0ee7dd-7778-4689-fc4e-e6d68c5da6bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["If you don't want to specify a topic, press enter. Otherwise, type the topic you want:government\n","\n"]}],"source":["while True:\n","  topic = input(\"If you don't want to specify a topic, press enter. Otherwise, type the topic you want:\")\n","  if topic not in theme_synonyms and topic != '':\n","    print(\"Sorry, it is not a valid topic. Please try again.\")\n","  else:\n","    break\n","print()"]},{"cell_type":"code","execution_count":287,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1685501956370,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"867euJBQfSu-"},"outputs":[],"source":["topic_synonyms = [syn for top in topic.split() for syn in get_theme_synonyms(top)]\n","\n","synonyms = theme_synonyms.union(topic_synonyms)\n","\n","freq_words_for_theme = get_frequent_words(theme.lower(), 50)"]},{"cell_type":"code","execution_count":288,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685501956370,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"2sh-15A5nVl6","outputId":"93f4538a-11ba-43f6-cfff-bb60b68b3ca2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["783"]},"metadata":{},"execution_count":288}],"source":["len(freq_words_for_theme)"]},{"cell_type":"code","execution_count":289,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2274,"status":"ok","timestamp":1685501960785,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"yd34l0AahlhL","outputId":"71eed622-2b7b-4ffc-b641-141172433b7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Select the letter for alliteration:\n","Your alliteration letter will be: \n","0\n"]}],"source":["alliteration = input(\"Select the letter for alliteration:\")\n","\n","\n","print(\"Your alliteration letter will be:\", alliteration)\n","\n","alliteration_words = alliteration_freq_words(alliteration, freq_words_for_theme)\n","\n","if alliteration == '' : alliteration_words = []\n","print(len(alliteration_words))"]},{"cell_type":"code","execution_count":290,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1953,"status":"ok","timestamp":1685501963671,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"goOoGVjjhuax","outputId":"64b51eee-9a0e-4b87-9111-1d1809f99cd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["<function trochaic_stress_match at 0x7fec86b075b0> 197\n"]}],"source":["rhythm_functions = [iambic_stress_match, trochaic_stress_match, dactylic_stress_match, anapestic_stress_match]\n","rhythm_candidates = []\n","for func in rhythm_functions:\n","  rhythm_candidates.append(rhythm_word_candidates(func, freq_words_for_theme))\n","\n","rhythm, rhythm_words = max([rhythm for rhythm in enumerate(rhythm_candidates)], key = lambda x: len(x[1]))\n","\n","print(rhythm_functions[rhythm], len(rhythm_words))\n","\n","rhytm_func = rhythm_functions[rhythm]"]},{"cell_type":"markdown","metadata":{"id":"gzZHc8fqjvoQ"},"source":["Checking results"]},{"cell_type":"code","execution_count":291,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1685501966819,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"JY5ZhWNWjxBB","outputId":"2f59875b-5c51-40ba-9335-050f4a542d64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Portion of alliteration words:\n","\n","Portion of rhythm words:\n","ancient             angels              answer              \n","army                baby                battle              \n","beauty              bloody              bodies              \n","\n","Portion of synonym words:\n","political sympathiespolitics            political relation  \n","political science   authorities         administration      \n","government activity regime              governing           \n","\n","Portion of frequent words:\n","i                   time                white               \n","night               world               day                 \n","life                light               black               \n","\n"]}],"source":["print(\"Portion of alliteration words:\")\n","for (i, word) in enumerate(alliteration_words[:9]):\n","  print(\"{:<20}\".format(word), end= \"\")\n","  if (i+1)%3 == 0 : print()\n","print()\n","\n","print(\"Portion of rhythm words:\")\n","for (i, word) in enumerate(rhythm_words[:9]):\n","  print(\"{:<20}\".format(word), end= \"\")\n","  if (i+1)%3 == 0 : print()\n","print()\n","\n","print(\"Portion of synonym words:\")\n","for (i, word) in enumerate(list(synonyms)[:9]):\n","  print(\"{:<20}\".format(word), end= \"\")\n","  if (i+1)%3 == 0 : print()\n","print()\n","\n","print(\"Portion of frequent words:\")\n","for (i, word) in enumerate(freq_words_for_theme[:9]):\n","  print(\"{:<20}\".format(word), end= \"\")\n","  if (i+1)%3 == 0 : print()\n","print()"]},{"cell_type":"code","source":["brown_cfd_sum_avg = sum(brown_cfd[cond].N() for cond in brown_cfd.conditions()) / len(brown_cfd.conditions())\n","poem_cfd_sum_avg = sum(poem_cfd[cond].N() for cond in poem_cfd.conditions()) / len(poem_cfd.conditions())\n","\n","def calc_continuity(next_pair, brown):\n","  if brown:\n","    proportion = (float(brown_cfd[next_pair].N()) / brown_cfd_sum_avg) * 10000\n","    if proportion> 1: return 1\n","    else: return proportion \n","  else:\n","    proportion = (float(poem_cfd[next_pair].N()) / poem_cfd_sum_avg) * 20000\n","    if proportion> 1: return 1\n","    else: return proportion"],"metadata":{"id":"5pGT84i-OAjI","executionInfo":{"status":"ok","timestamp":1685501967907,"user_tz":-540,"elapsed":755,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"}}},"execution_count":292,"outputs":[]},{"cell_type":"code","execution_count":293,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1685501967908,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"d00F3v3PbcDS"},"outputs":[],"source":["def generate_word(pair, sent):\n","  word_rank = defaultdict(float)\n","\n","  for word in brown_cfd[pair].keys():\n","    if word.isalpha(): \n","      brown_total =  (0.4 + brown_cfd[pair].freq(word) + calc_continuity((pair[-1], word), True))\n","      word_rank[(pair,word)] += brown_total\n","    else: continue\n","\n","  for word in poem_cfd[pair].keys(): \n","    if word.isalpha():\n","      poem_total = (0.45 + poem_cfd[pair].freq(word) + calc_continuity((pair[-1], word), False))\n","      word_rank[(pair,word)] += poem_total\n","    else: continue\n","\n","  all_pairs = set(brown_cfd.conditions()).union(set((poem_cfd.conditions())))\n","  first_word = set(w1 for (w1,w2) in all_pairs)\n","  \n","  for key in word_rank.keys():\n","    pair, word = key\n","    #not in frequent words but satisfies alliteration and rhythm function\n","    if word.startswith(alliteration): word_rank[key] += 0.2\n","\n","    if rhytm_func(word): word_rank[(pair,word)] += 0.1\n","\n","    if word in alliteration_words:\n","      word_rank[key] += 0.3\n","\n","    if word in rhythm_words:\n","      word_rank[key] += 0.3\n","\n","    split_synonyms = [s for syn in synonyms for s in syn.split()]\n","\n","    if word in synonyms:\n","      word_rank[key] += 0.4\n","    if word in freq_words_for_theme:\n","      word_rank[key] += 0.3\n","    \n","    if word in sent and len(word)>3:\n","      word_rank[key] -= 10\n","    \n","    next_pair = (pair[-1], word)\n","    \n","    if next_pair not in all_pairs or word not in first_word:\n","      word_rank[key] -= 1000\n","\n","  max_rank = max(list(word_rank.keys()), key= lambda x: word_rank[x])\n","  \n","  print(max_rank[1], word_rank[max_rank])\n","  return (max_rank[1], word_rank[max_rank])"]},{"cell_type":"code","execution_count":294,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685501967908,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"0iCF-cUfb1da","outputId":"c7e09918-8e94-4a63-bceb-7a2e48686f41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\npair = ('the','sun')\\nsent = ['the', 'sun']\\nfor j in range(10):\\n    next_word = generate_word(pair)\\n    pair = (pair[-1], next_word)\\n    sent.append(next_word)\\n    print(next_word)\\n    if pair in common_line_endings:\\n      break\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":294}],"source":["'''\n","pair = ('the','sun')\n","sent = ['the', 'sun']\n","for j in range(10):\n","    next_word = generate_word(pair)\n","    pair = (pair[-1], next_word)\n","    sent.append(next_word)\n","    print(next_word)\n","    if pair in common_line_endings:\n","      break\n","'''"]},{"cell_type":"code","execution_count":295,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685501967908,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"oUCAVGrJefk6","outputId":"7f67a546-f7f2-4870-cb8e-d6fd241231bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['political',\n"," 'sympathies',\n"," 'politics',\n"," 'political',\n"," 'relation',\n"," 'political',\n"," 'science',\n"," 'authorities',\n"," 'administration',\n"," 'government',\n"," 'activity',\n"," 'regime',\n"," 'governing',\n"," 'governance',\n"," 'government']"]},"metadata":{},"execution_count":295}],"source":["synonym_list = list(syn for synonym in synonyms for syn in synonym.split() if len(syn)>3)\n","synonym_list"]},{"cell_type":"code","execution_count":296,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685501969301,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"},"user_tz":-540},"id":"5BeVYliOZLwr"},"outputs":[],"source":["import random\n","\n","def random_start_pair(choice):\n","  conditions= poem_cfd.conditions()\n","  pairs = [pair for pair in conditions if choice in pair and len(poem_cfd[pair].keys())>3]\n","  return pairs"]},{"cell_type":"code","source":["def word_count(poem):\n","  return sum(len(line) for line in poem)"],"metadata":{"id":"-rVr9z4ceJia","executionInfo":{"status":"ok","timestamp":1685501970192,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"}}},"execution_count":297,"outputs":[]},{"cell_type":"code","execution_count":302,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcZjmPcur0t8","executionInfo":{"status":"ok","timestamp":1685502213143,"user_tz":-540,"elapsed":41042,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"}},"outputId":"393b6343-61d2-475e-e415-76337c732079"},"outputs":[{"output_type":"stream","name":"stdout","text":["['coming', 'he']\n","turned 2.9\n","to 3.548562392179414\n","look 3.4164265706282513\n","coming he turned to look\n","at 3.6436613055818357\n","the 3.5615083697234353\n","moment 3.672438652386497\n","i 3.47346818868558\n","was 3.5619565217391305\n","ready 3.662598142760352\n","to 4.242982456140351\n","go 3.4207098679959382\n","at the moment i was ready to go\n","home 3.4487854251012147\n","and 3.333834586466166\n","set 3.4275151181968115\n","the 3.3304597701149428\n","table 3.7479710144927534\n","home and set the table\n","and 3.2645512928849083\n","the 3.2357142857142858\n","people 3.656270783528894\n","are 3.393793390724799\n","leaving 3.7035135135135135\n","things 2.1166666666666667\n","we 2.6500000000000004\n","do 3.6799999999999997\n","and the people are leaving things we do\n","not 3.8476998904709747\n","have 3.424917719943579\n","to 3.4012238041913294\n","carry 3.6588330124042807\n","out 3.4396561857184835\n","the 3.7474789915966387\n","window 3.8170143346170304\n","not have to carry out the window\n","and 3.3411458333333335\n","watch 3.458669108669109\n","\n","\n","coming he turned to look\n","at the moment i was ready to go\n","home and set the table\n","and the people are leaving things we do\n","not have to carry out the window\n","Score: 0.7718245080822331\n"]}],"source":["MAX_LENGTH = 10\n","NUM_LINES = 5\n","\n","score = 0\n","\n","choice = random.choice(freq_words_for_theme)\n","\n","pairs = random_start_pair(choice)\n","if pairs:\n","  pair = random.choice(pairs)\n","else:\n","  pair = random.choice(list(poem_cfd.conditions()))\n","\n","\n","prev_pair = ('~~','~')\n","poem = []\n","sent = list(pair)\n","print(sent)\n","for i in range(NUM_LINES):\n","  for j in range(10):\n","      next_word, next_score = generate_word(pair, set(sent))\n","      score += next_score\n","      pair = (pair[-1], next_word)\n","      sent.append(next_word)\n","      #print(next_word)\n","      if len(sent)> 4 and pair in common_line_endings:\n","        if len(poem) == 0: MAX_LENGTH = len(sent)\n","        line = \" \".join(sent)\n","        print(line)\n","        poem.append(line)\n","        next_word_1, next_score1 = generate_word(pair, set(sent))\n","        score += next_score1\n","        pair = (pair[-1], next_word_1)\n","        next_word_2, next_score2 = generate_word(pair, set(sent))\n","        score+= next_score2\n","        pair = (next_word_1, next_word_2)\n","        sent = [next_word_1,next_word_2]\n","        break\n","      elif j == MAX_LENGTH:\n","        line = \" \".join(sent)\n","        print(line)\n","        poem.append(line)\n","        next_word_1, next_score1 = generate_word(pair, set(sent))\n","        score += next_score1\n","        pair = (pair[-1], next_word_1)\n","        next_word_2, next_score2 = generate_word(pair, set(sent))\n","        score += next_score2\n","        pair = (next_word_1, next_word_2)\n","        sent = [next_word_1,next_word_2]\n","      \n","        \n","print()\n","print()\n","for line in poem:\n","  print(line)\n","\n","count = word_count(poem)\n","\n","print(\"Score:\", score/count)\n","  "]},{"cell_type":"code","source":[],"metadata":{"id":"4i44e5LGskDW","executionInfo":{"status":"ok","timestamp":1685501884949,"user_tz":-540,"elapsed":406,"user":{"displayName":"Yokyung Lee","userId":"11406225766971149967"}}},"execution_count":283,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJbwHkcmMkZmJrGty4jQ55"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}